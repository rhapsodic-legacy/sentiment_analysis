{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Ky0ZcRrH6KWovisAb78I_cUdNfS8U__0","timestamp":1679809384360}],"authorship_tag":"ABX9TyP0ZWwebXnU1b6s5u8JXz7u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Text Sentiment Analysis\n","\n","#### This is version eight \n","\n"," This script preprocesses the IMDB movie review data, trains a machine learning model, and deploys it to the web via Flask to allow users to input text and get a sentiment classification result.\n","\n","Preprocessing the IMDB movie review dataset from Stanford:\n","+ Download the dataset from the Stanford AI Group website (http://ai.stanford.edu/~amaas/data/sentiment/)\n","+ Extract the files and preprocess the data by removing HTML tags, converting all text to lowercase, removing punctuation and special characters, and tokenizing the text into individual words.\n","+ Split the preprocessed data into training and testing sets.\n","\n","Running the preprocessed data through a Naive Bayes model:\n","+ Train a Naive Bayes model on the preprocessed training data.\n","+ Use the trained model to predict the sentiment of the preprocessed testing data.\n","+ Evaluate the performance of the model using metrics like accuracy, precision, recall, and F1 score.\n","\n","Deploying the model to the internet via Flask:\n","+ Create a Flask app that allows users to input a line of text to be classified.\n","Preprocess the user input by removing HTML tags, converting to lowercase, removing punctuation and special characters, and tokenizing the text into individual words.\n","+ Use the trained Naive Bayes model to classify the preprocessed user input as either positive or negative.\n","+ Return the classification result to the user via the Flask app.\n"],"metadata":{"id":"yGiU1tsEa9ay"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ge9tWPtobJtx","executionInfo":{"status":"ok","timestamp":1679884789179,"user_tz":240,"elapsed":24543,"user":{"displayName":"Dream Phoenix","userId":"09226332887600260845"}},"outputId":"d4ddb029-869e-4ad4-9731-180b9bc7144c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import string\n","\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","\n","def load_imdb_data(path):\n","    data = []\n","    labels = []\n","\n","    for sentiment in [\"neg\", \"pos\"]:\n","        dir_path = os.path.join(path, sentiment)\n","        for filename in os.listdir(dir_path):\n","            with open(os.path.join(dir_path, filename), \"r\", encoding=\"utf-8\") as f:\n","                data.append(f.read())\n","                labels.append(1 if sentiment == \"pos\" else 0)\n","\n","    return pd.DataFrame({\"review\": data, \"label\": labels})\n","\n","def preprocess_text(text):\n","    # Lowercase the text\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n","\n","    # Tokenize the text\n","    words = word_tokenize(text)\n","\n","    # Remove stopwords\n","    words = [word for word in words if word not in stopwords.words(\"english\")]\n","\n","    # Lemmatize the words\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    return \" \".join(words)\n","\n","if __name__ == \"__main__\":\n","    #data_path = \"aclImdb\"\n","    data_path = \"/content/drive/MyDrive/aclImdb\"\n","    train_path = os.path.join(data_path, \"train\")\n","    test_path = os.path.join(data_path, \"test\")\n","\n","    train_df = load_imdb_data(train_path)\n","    test_df = load_imdb_data(test_path)\n","\n","    train_df[\"review\"] = train_df[\"review\"].apply(preprocess_text)\n","    test_df[\"review\"] = test_df[\"review\"].apply(preprocess_text)\n","\n","    train_df.to_csv(\"preprocessed_train.csv\", index=False)\n","    test_df.to_csv(\"preprocessed_test.csv\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAaUIxfJcjqt","executionInfo":{"status":"ok","timestamp":1679887727529,"user_tz":240,"elapsed":2820468,"user":{"displayName":"Dream Phoenix","userId":"09226332887600260845"}},"outputId":"bb9dd2d2-2a61-4477-84a2-cb4a1dd6bceb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["# Copy the data to your local drive to opvoid the need to preprocessing again.\n","!cp /content/preprocessed_test.csv /content/drive/MyDrive/aclImdb\n","!cp /content/preprocessed_train.csv /content/drive/MyDrive/aclImdb"],"metadata":{"id":"J7zUBlsGbgAn","executionInfo":{"status":"ok","timestamp":1679888528390,"user_tz":240,"elapsed":588,"user":{"displayName":"Dream Phoenix","userId":"09226332887600260845"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report\n","import csv\n","\n","def load_data(train_file, test_file):\n","    train_df = pd.read_csv(train_file)\n","    test_df = pd.read_csv(test_file)\n","\n","    return train_df, test_df\n","\n","def create_vectorizer():\n","    return TfidfVectorizer()\n","\n","def train_model(train_data, train_labels, vectorizer):\n","    X_train = vectorizer.fit_transform(train_data)\n","\n","    model = MultinomialNB()\n","    param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0]}\n","    grid_search = GridSearchCV(model, param_grid, cv=5)\n","    grid_search.fit(X_train, train_labels)\n","\n","    return grid_search.best_estimator_\n","\n","def evaluate_model(model, test_data, test_labels, vectorizer):\n","    X_test = vectorizer.transform(test_data)\n","\n","    predictions = model.predict(X_test)\n","    print(classification_report(test_labels, predictions))\n","\n","    return predictions\n","\n","def save_predictions(predictions, output_file):\n","    with open(output_file, \"w\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"prediction\"])\n","        for pred in predictions:\n","            writer.writerow([pred])\n","\n","if __name__ == \"__main__\":\n","    train_file = \"preprocessed_train.csv\"\n","    test_file = \"preprocessed_test.csv\"\n","    predictions_file = \"predictions.csv\"\n","\n","    train_df, test_df = load_data(train_file, test_file)\n","\n","    vectorizer = create_vectorizer()\n","    model = train_model(train_df[\"review\"], train_df[\"label\"], vectorizer)\n","\n","    predictions = evaluate_model(model, test_df[\"review\"], test_df[\"label\"], vectorizer)\n","    save_predictions(predictions, predictions_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FGW6T-dqXK6J","executionInfo":{"status":"ok","timestamp":1679892187978,"user_tz":240,"elapsed":10071,"user":{"displayName":"Dream Phoenix","userId":"09226332887600260845"}},"outputId":"3b6194b6-f07f-45c9-ccee-791e8d6c7717"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.81      0.89      0.84     12500\n","           1       0.88      0.79      0.83     12500\n","\n","    accuracy                           0.84     25000\n","   macro avg       0.84      0.84      0.84     25000\n","weighted avg       0.84      0.84      0.84     25000\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import pickle\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report\n","import csv\n","\n","def load_data(train_file, test_file):\n","    train_df = pd.read_csv(train_file)\n","    test_df = pd.read_csv(test_file)\n","\n","    return train_df, test_df\n","\n","def create_vectorizer():\n","    return TfidfVectorizer()\n","\n","def train_model(train_data, train_labels, vectorizer):\n","    X_train = vectorizer.fit_transform(train_data)\n","\n","    model = MultinomialNB()\n","    param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0]}\n","    grid_search = GridSearchCV(model, param_grid, cv=5)\n","    grid_search.fit(X_train, train_labels)\n","\n","    return grid_search.best_estimator_\n","\n","def evaluate_model(model, test_data, test_labels, vectorizer):\n","    X_test = vectorizer.transform(test_data)\n","\n","    predictions = model.predict(X_test)\n","    print(classification_report(test_labels, predictions))\n","\n","    return predictions\n","\n","def save_predictions(predictions, output_file):\n","    with open(output_file, \"w\", newline=\"\") as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"prediction\"])\n","        for pred in predictions:\n","            writer.writerow([pred])\n","\n","if __name__ == \"__main__\":\n","    train_file = \"preprocessed_train.csv\"\n","    test_file = \"preprocessed_test.csv\"\n","    predictions_file = \"predictions.csv\"\n","\n","    train_df, test_df = load_data(train_file, test_file)\n","\n","    vectorizer = create_vectorizer()\n","    model = train_model(train_df[\"review\"], train_df[\"label\"], vectorizer)\n","\n","    # Save the trained model and vectorizer\n","    with open(\"model.pkl\", \"wb\") as model_file:\n","        pickle.dump(model, model_file)\n","\n","    with open(\"vectorizer.pkl\", \"wb\") as vectorizer_file:\n","        pickle.dump(vectorizer, vectorizer_file)\n","\n","    predictions = evaluate_model(model, test_df[\"review\"], test_df[\"label\"], vectorizer)\n","    save_predictions(predictions, predictions_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42qhTNH5Yvjd","executionInfo":{"status":"ok","timestamp":1679892364645,"user_tz":240,"elapsed":13988,"user":{"displayName":"Dream Phoenix","userId":"09226332887600260845"}},"outputId":"4f4e5a07-88d0-4d31-ba70-034f660c87d6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.81      0.89      0.84     12500\n","           1       0.88      0.79      0.83     12500\n","\n","    accuracy                           0.84     25000\n","   macro avg       0.84      0.84      0.84     25000\n","weighted avg       0.84      0.84      0.84     25000\n","\n"]}]},{"cell_type":"code","source":["# Copy the data to your local drive to opvoid the need to run the model again.\n","!cp /content/model.pkl /content/drive/MyDrive/aclImdb\n","!cp /content/vectorizer.pkl /content/drive/MyDrive/aclImdb"],"metadata":{"id":"z_SlTR-5Y4QJ","executionInfo":{"status":"ok","timestamp":1679892425718,"user_tz":240,"elapsed":405,"user":{"displayName":"Dream Phoenix","userId":"09226332887600260845"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0Tr9azaxZGzL"},"execution_count":null,"outputs":[]}]}